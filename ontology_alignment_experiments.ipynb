{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ontology Alignment Experiments: Graph Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates a complete graph analysis pipeline for ontology alignment:\n",
    "1. **Exploratory Data Analysis (EDA)** - Understanding the graph structure\n",
    "2. **Weakly Connected Components (WCC)** - Identifying connected subgraphs\n",
    "3. **Node Similarity** - Finding similar nodes based on shared properties\n",
    "4. **Node Embeddings** - Creating vector representations for similarity analysis"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Setup for Google Colab (skip if running locally)\n",
    "try:\n",
    "    import google.colab\n",
    "    import os\n",
    "    print(\"ðŸ“¦ Setting up Colab environment...\\n\")\n",
    "    GITHUB_URL = \"https://github.com/teutaD/EntityMatching-experimentation.git\"\n",
    "\n",
    "    # Clone repository\n",
    "    !git clone {GITHUB_URL}\n",
    "\n",
    "    # Check if clone was successful\n",
    "    if os.path.exists('EntityMatching-experimentation'):\n",
    "        os.chdir('EntityMatching-experimentation')\n",
    "        print(\"âœ… Repository cloned successfully\\n\")\n",
    "    else:\n",
    "        raise Exception(\"Failed to clone repository. Make sure the URL is correct and the repo is public.\")\n",
    "\n",
    "    # Install dependencies\n",
    "    print(\"ðŸ“¦ Installing dependencies...\")\n",
    "    !pip install -q neo4j pandas matplotlib seaborn\n",
    "    print(\"âœ… Setup complete!\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"âœ… Running locally - no setup needed\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during setup: {e}\")\n",
    "    print(\"\\nðŸ’¡ Solutions:\")\n",
    "    print(\"   1. Make sure your GitHub repository is PUBLIC\")\n",
    "    print(\"   2. Or use a personal access token for private repos (see Option 2 above)\")\n",
    "    print(\"   3. Or manually upload the project files using Colab's file browser\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Neo4j Database Setup\n",
    "\n",
    "**Before running this notebook, you need a Neo4j database instance.**\n",
    "\n",
    "### Option 1: Use Neo4j Sandbox (Recommended for Colab)\n",
    "1. Go to [Neo4j Sandbox](https://sandbox.neo4j.com/)\n",
    "2. Create a free account and launch a blank sandbox\n",
    "3. Click **\"Connection details\"** and download the credentials file\n",
    "4. Open the downloaded `.txt` file and copy the values below:\n",
    "   - **Bolt URL** (e.g., `bolt://54.123.45.67:7687` or `neo4j+s://xxxxx.databases.neo4j.io`)\n",
    "   - **Username** (usually `neo4j`)\n",
    "   - **Password** (provided in the file)\n",
    "\n",
    "### Option 2: Use Your Own Neo4j Instance\n",
    "If you have your own Neo4j database (local or cloud), use those credentials instead.\n",
    "\n",
    "âš ï¸ **Important**: Make sure your Neo4j instance:\n",
    "- Has the **Graph Data Science (GDS) library** installed\n",
    "- Is accessible from the internet (if using Colab)\n",
    "- Has the necessary data loaded"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ðŸ” Neo4j Connection Credentials\n",
    "# Update these values with your Neo4j Sandbox or instance credentials\n",
    "\n",
    "NEO4J_URI = \"bolt://44.200.0.31\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"gyroscopes-alibis-prints\"\n",
    "\n",
    "# Test connection\n",
    "print(\"Testing Neo4j connection...\")\n",
    "try:\n",
    "    from neo4j import GraphDatabase\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    driver.verify_connectivity()\n",
    "    driver.close()\n",
    "    print(\"âœ… Successfully connected to Neo4j!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Connection failed: {e}\")\n",
    "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
    "    print(\"   1. Verify your credentials are correct\")\n",
    "    print(\"   2. Check if the Neo4j instance is running\")\n",
    "    print(\"   3. Ensure port 7687 is accessible (firewall/security groups)\")\n",
    "    print(\"   4. For Colab: Use Neo4j Sandbox or a cloud-hosted instance\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup and Imports"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install neo4j ydata_profiling"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from neo4j import GraphDatabase\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "\n",
    "# Import EDA analyzer - add eda directory to path\n",
    "eda_path = os.path.join(os.getcwd(), 'eda')\n",
    "if eda_path not in sys.path:\n",
    "    sys.path.insert(0, eda_path)\n",
    "    print(f\"âœ… Added {eda_path} to Python path\")\n",
    "\n",
    "# Import modules\n",
    "try:\n",
    "    from neo4j_analyzer import Neo4jPropertyAnalyzer, PerformanceMonitor\n",
    "    from neo4j_analyzer.report_generator import ReportGenerator\n",
    "    print(\"âœ… Successfully imported neo4j_analyzer\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(f\"Directory contents: {os.listdir('.')}\")\n",
    "    if os.path.exists('eda'):\n",
    "        print(f\"EDA directory contents: {os.listdir('eda')}\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "\n",
    "**Analysis Settings:**\n",
    "(Neo4j credentials were set in the previous cell)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analysis Settings\n",
    "USE_FAST_MODE = True      # Use Cypher aggregations for large graphs\n",
    "SAMPLE_SIZE = 50000       # Sample size for standard mode\n",
    "FETCH_SIZE = 2000         # Batch size for data extraction\n",
    "\n",
    "print(f\"Using Neo4j at: {NEO4J_URI}\")\n",
    "print(f\"Fast mode: {USE_FAST_MODE}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Part 1: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Understanding the structure and properties of our graph data before running algorithms."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1 Initialize Analyzer and Explore Database"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize analyzer\n",
    "analyzer = Neo4jPropertyAnalyzer(\n",
    "    uri=NEO4J_URI,\n",
    "    user=NEO4J_USER,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    fetch_size=FETCH_SIZE\n",
    ")\n",
    "\n",
    "# Get all node labels in the database\n",
    "labels = analyzer.get_node_labels()\n",
    "print(f\"Found {len(labels)} node labels in the database:\")\n",
    "for label in labels:\n",
    "    count = analyzer.get_node_count(label)\n",
    "    print(f\"  - {label}: {count:,} nodes\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.2 Analyze Node Properties\n",
    "\n",
    "Analyze properties to understand:\n",
    "- **Categorical properties**: Low cardinality, good for grouping\n",
    "- **Unique properties**: High cardinality, good for identifiers\n",
    "- **Property distributions**: Understanding data quality"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analyze properties for each label\n",
    "all_results = {}\n",
    "\n",
    "for label in labels:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing label: {label}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if USE_FAST_MODE:\n",
    "        summary = analyzer.get_property_summary_fast(label)\n",
    "    else:\n",
    "        summary = analyzer.get_property_summary(label, sample_size=SAMPLE_SIZE)\n",
    "\n",
    "    all_results[label] = summary\n",
    "    ReportGenerator.print_summary(summary, label)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.3 Visualize Property Types Distribution"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Aggregate property types across all labels\n",
    "property_type_counts = {}\n",
    "\n",
    "for label, summary in all_results.items():\n",
    "    for prop_name, prop_info in summary.items():\n",
    "        prop_type = prop_info.get('type', 'UNKNOWN')\n",
    "        if prop_type not in property_type_counts:\n",
    "            property_type_counts[prop_type] = 0\n",
    "        property_type_counts[prop_type] += 1\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "ax1.pie(property_type_counts.values(), labels=property_type_counts.keys(),\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Property Types Distribution')\n",
    "\n",
    "# Bar chart\n",
    "ax2.bar(property_type_counts.keys(), property_type_counts.values())\n",
    "ax2.set_xlabel('Property Type')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Property Types Count')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Part 2: Weakly Connected Components (WCC)\n",
    "\n",
    "**Algorithm**: WCC identifies groups of nodes that are connected to each other, even if the connections are indirect.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `GRAPH_NAME`: Name of the GDS graph projection to analyze\n",
    "- `USER_LABEL`: Node label to focus on (e.g., 'Stream')\n",
    "- `PROPERTY_LABEL`: Related property nodes (e.g., 'Property')\n",
    "- `REL_TYPE`: Relationship type connecting nodes (e.g., 'HAS')\n",
    "\n",
    "**What it does:**\n",
    "- Finds all connected components in the graph\n",
    "- Assigns a `component_id` to each node\n",
    "- Filters components with size > 1 (groups with multiple nodes)\n",
    "- Writes `component_id_2` property to Stream nodes for downstream analysis"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# WCC Configuration\n",
    "# Graph projection settings - customize these based on your data\n",
    "GRAPH_NAME = \"stream-user-team-graph\"\n",
    "LABELS = [\"Stream\", \"Team\", \"Game\"]\n",
    "REL_TYPES = [\"VIP\", \"HAS_TEAM\"]\n",
    "\n",
    "print(f\"Graph Projection Configuration:\")\n",
    "print(f\"  Graph name: {GRAPH_NAME}\")\n",
    "print(f\"  Node labels: {LABELS}\")\n",
    "print(f\"  Relationship: {REL_TYPES})\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Graph Projection (if it doesn't exist)\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "with driver:\n",
    "    with driver.session() as session:\n",
    "        # Check if graph already exists\n",
    "        result = session.run(\n",
    "            \"CALL gds.graph.exists($name) YIELD exists RETURN exists\",\n",
    "            name=GRAPH_NAME\n",
    "        )\n",
    "        graph_exists = result.single()[\"exists\"]\n",
    "\n",
    "        if graph_exists:\n",
    "            print(f\"âœ… Graph '{GRAPH_NAME}' already exists\")\n",
    "\n",
    "            # Optionally drop and recreate if you want to update the projection\n",
    "            # Uncomment the following lines to recreate:\n",
    "            # print(f\"Dropping existing graph...\")\n",
    "            # session.run(\"CALL gds.graph.drop($name)\", name=GRAPH_NAME)\n",
    "            # graph_exists = False\n",
    "\n",
    "        if not graph_exists:\n",
    "            print(f\"Creating graph projection '{GRAPH_NAME}'...\")\n",
    "            print(f\"  Node labels: {LABELS}\")\n",
    "            print(f\"  Relationship: {REL_TYPES})\")\n",
    "\n",
    "            # Create the graph projection using Cypher projection\n",
    "            session.run(\n",
    "                \"CALL gds.graph.project($name, $nodeLabels, $relConfig)\",\n",
    "                name=GRAPH_NAME,\n",
    "                nodeLabels=LABELS,\n",
    "                relConfig=REL_TYPES\n",
    "            )\n",
    "\n",
    "            print(f\"âœ… Graph projection '{GRAPH_NAME}' created successfully\")\n",
    "\n",
    "            # Show graph info\n",
    "            info = session.run(\n",
    "                \"CALL gds.graph.list($name) \"\n",
    "                \"YIELD graphName, nodeCount, relationshipCount \"\n",
    "                \"RETURN graphName, nodeCount, relationshipCount\",\n",
    "                name=GRAPH_NAME\n",
    "            ).single()\n",
    "\n",
    "            if info:\n",
    "                print(f\"\\nðŸ“Š Graph Statistics:\")\n",
    "                print(f\"  Nodes: {info['nodeCount']:,}\")\n",
    "                print(f\"  Relationships: {info['relationshipCount']:,}\")\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run WCC Algorithm\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "with driver:\n",
    "    with driver.session() as session:\n",
    "        # Step 1: Run WCC and write component IDs to nodes\n",
    "        print(\"Running WCC algorithm and writing component_id to nodes...\")\n",
    "        session.run(\n",
    "            \"CALL gds.wcc.stream($name) \"\n",
    "            \"YIELD nodeId, componentId \"\n",
    "            \"WITH gds.util.asNode(nodeId) AS n, componentId \"\n",
    "            \"WHERE n:Stream \"\n",
    "            \"WITH componentId, collect(n) AS nodes, count(*) AS size \"\n",
    "            \"WHERE size > 1 \"\n",
    "            \"UNWIND nodes AS n \"\n",
    "            \"SET n.component_id = componentId\",\n",
    "            name=GRAPH_NAME,\n",
    "        )\n",
    "        print(\"âœ… Component IDs written to nodes\")\n",
    "\n",
    "        # Step 2: Query component statistics (one row per component)\n",
    "        result = session.run(\n",
    "            \"CALL gds.wcc.stream($name) \"\n",
    "            \"YIELD nodeId, componentId \"\n",
    "            \"WITH gds.util.asNode(nodeId) AS n, componentId \"\n",
    "            \"WHERE n:Stream \"\n",
    "            \"WITH componentId, count(*) AS size \"\n",
    "            \"WHERE size > 1 \"\n",
    "            \"RETURN componentId, size \"\n",
    "            \"ORDER BY size DESC, componentId ASC\",\n",
    "            name=GRAPH_NAME,\n",
    "        )\n",
    "\n",
    "        # Collect results (one row per component)\n",
    "        wcc_results = []\n",
    "        for record in result:\n",
    "            wcc_results.append({\n",
    "                'component_id': record['componentId'],\n",
    "                'size': record['size']\n",
    "            })\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# Display results\n",
    "wcc_df = pd.DataFrame(wcc_results)\n",
    "print(f\"\\nFound {len(wcc_df)} connected components with size > 1\")\n",
    "print(f\"Total nodes in components: {wcc_df['size'].sum():,}\")\n",
    "display(wcc_df.head(10))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1 Visualize Component Size Distribution"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize component sizes\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Histogram of component sizes\n",
    "plt.hist(wcc_df['size'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Component Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Component Sizes')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nComponent Statistics:\")\n",
    "print(f\"  Mean size: {wcc_df['size'].mean():.2f}\")\n",
    "print(f\"  Median size: {wcc_df['size'].median():.2f}\")\n",
    "print(f\"  Largest component: {wcc_df['size'].max():,} nodes\")\n",
    "print(f\"  Smallest component: {wcc_df['size'].min():,} nodes\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Part 3: Node Similarity\n",
    "\n",
    "**Algorithm**: Node Similarity finds pairs of nodes that are similar based on their shared neighbors (Jaccard similarity).\n",
    "\n",
    "**Key Parameters:**\n",
    "- `GRAPH_NAME`: GDS graph projection (must include component_id property from previous step)\n",
    "- `NODE_LABELS`: Filter to specific node types (e.g., ['Stream'])\n",
    "- `THRESHOLD`: Minimum score to consider nodes for string matching\n",
    "- `similarity > 0`: Only return node pairs with non-zero similarity\n",
    "\n",
    "**What it does:**\n",
    "- Compares nodes based on shared properties/neighbors\n",
    "- Calculates Jaccard similarity: |A âˆ© B| / |A âˆª B|\n",
    "- Returns pairs of similar nodes with similarity scores\n",
    "- Useful for finding potential duplicates or related entities"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Node Similarity Configuration\n",
    "SIMILARITY_GRAPH_NAME = \"stream-user-team-graph\"\n",
    "NODE_LABELS = [\"Stream\"]\n",
    "THRESHOLD = 0.2\n",
    "\n",
    "print(f\"Running Node Similarity on graph: {SIMILARITY_GRAPH_NAME}\")\n",
    "print(f\"Analyzing node labels: {NODE_LABELS}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run Node Similarity Algorithm (Only Within Same Component)\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "with driver:\n",
    "    with driver.session() as session:\n",
    "        print(\"ðŸ” Computing similarity only for nodes in the same component...\")\n",
    "\n",
    "        result = session.run(\n",
    "            \"CALL gds.nodeSimilarity.stream($name, { \"\n",
    "            \"  nodeLabels: $labels \"\n",
    "            \"}) \"\n",
    "            \"YIELD node1, node2, similarity \"\n",
    "            \"WITH gds.util.asNode(node1) AS n1, gds.util.asNode(node2) AS n2, similarity \"\n",
    "            \"WHERE similarity > 0 \"\n",
    "            \"  AND n1.component_id IS NOT NULL \"\n",
    "            \"  AND n2.component_id IS NOT NULL \"\n",
    "            \"  AND n1.component_id = n2.component_id \"  # âš ï¸ Only same component!\n",
    "            \"RETURN n1.id AS node1_id, \"\n",
    "            \"       n1.component_id AS node1_component_id, \"\n",
    "            \"       n2.id AS node2_id, \"\n",
    "            \"       n2.component_id AS node2_component_id, \"\n",
    "            \"       similarity \"\n",
    "            \"ORDER BY similarity DESC \"\n",
    "            \"LIMIT 1000\",  # Limit for notebook display\n",
    "            name=SIMILARITY_GRAPH_NAME,\n",
    "            labels=NODE_LABELS,\n",
    "        )\n",
    "\n",
    "        # Collect results\n",
    "        similarity_results = []\n",
    "        for record in result:\n",
    "            similarity_results.append({\n",
    "                'node1_id': record['node1_id'],\n",
    "                'node1_component': record['node1_component_id'],\n",
    "                'node2_id': record['node2_id'],\n",
    "                'node2_component': record['node2_component_id'],\n",
    "                'similarity': record['similarity']\n",
    "            })\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# Display results\n",
    "similarity_df = pd.DataFrame(similarity_results)\n",
    "print(f\"\\nâœ… Found {len(similarity_df)} similar node pairs within same components (showing top 1000)\")\n",
    "print(f\"   All pairs share the same component_id\")\n",
    "high_score = similarity_df.loc[similarity_df[\"similarity\"] > THRESHOLD]\n",
    "display(high_score.head(20))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1 Visualize Similarity Distribution"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize similarity scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Histogram of similarity scores\n",
    "plt.hist(high_score['similarity'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Similarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Similarity Scores (Within Same Component)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSimilarity Statistics:\")\n",
    "print(f\"  Mean similarity: {high_score['similarity'].mean():.4f}\")\n",
    "print(f\"  Median similarity: {high_score['similarity'].median():.4f}\")\n",
    "print(f\"  Max similarity: {high_score['similarity'].max():.4f}\")\n",
    "print(f\"  Min similarity: {high_score['similarity'].min():.4f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.2 Component-Level Analysis\n",
    "\n",
    "**Note**: Since we computed similarity only for nodes within the same component, let's analyze which components have the most similar node pairs."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analyze similarity by component\n",
    "if len(similarity_df) > 0:\n",
    "    component_counts = similarity_df['node1_component'].value_counts().head(10)\n",
    "\n",
    "    print(f\"ðŸ“Š Component-Level Similarity Analysis:\")\n",
    "    print(f\"  Total similar pairs: {len(similarity_df):,}\")\n",
    "    print(f\"  Number of components with similar pairs: {similarity_df['node1_component'].nunique()}\")\n",
    "    print(f\"\\nðŸ” Top 10 Components by Number of Similar Pairs:\")\n",
    "    for comp_id, count in component_counts.items():\n",
    "        comp_data = similarity_df[similarity_df['node1_component'] == comp_id]\n",
    "        avg_sim = comp_data['similarity'].mean()\n",
    "        print(f\"  Component {comp_id}: {count} pairs (avg similarity: {avg_sim:.4f})\")\n",
    "else:\n",
    "    print(\"âš ï¸  No similar pairs found\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.3 String Similarity on High-Scoring Pairs\n",
    "\n",
    "For node pairs with high structural similarity, let's also compute **string similarity** on their text properties (e.g., `id`, `description`) to find potential duplicates or closely related entities."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# String Similarity Configuration\n",
    "SIMILARITY_THRESHOLD = 0.8  # âš ï¸ UPDATE: Only analyze pairs above this threshold\n",
    "STRING_PROPERTIES = [\"id\", \"description\"]  # âš ï¸ UPDATE: Properties to compare\n",
    "\n",
    "print(f\"String Similarity Configuration:\")\n",
    "print(f\"  Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(f\"  Properties to compare: {STRING_PROPERTIES}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter high-similarity pairs and fetch their properties\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def string_similarity(str1, str2):\n",
    "    \"\"\"Compute string similarity using SequenceMatcher (0.0 to 1.0)\"\"\"\n",
    "    if str1 is None or str2 is None:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, str(str1), str(str2)).ratio()\n",
    "\n",
    "# Filter to high-similarity pairs\n",
    "high_similarity_df = similarity_df[similarity_df['similarity'] >= SIMILARITY_THRESHOLD].copy()\n",
    "\n",
    "print(f\"\\nðŸ” Analyzing {len(high_similarity_df)} high-similarity pairs (similarity >= {SIMILARITY_THRESHOLD})\")\n",
    "\n",
    "if len(high_similarity_df) > 0:\n",
    "    # Fetch properties for these nodes\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "    with driver:\n",
    "        with driver.session() as session:\n",
    "            # Get unique node IDs\n",
    "            node_ids = set(high_similarity_df['node1_id'].tolist() + high_similarity_df['node2_id'].tolist())\n",
    "\n",
    "            print(f\"Fetching properties for {len(node_ids)} unique Stream nodes...\")\n",
    "\n",
    "            # Fetch properties\n",
    "            result = session.run(\n",
    "                \"MATCH (n:Stream) \"\n",
    "                \"WHERE n.id IN $ids \"\n",
    "                \"RETURN n.id AS id, \"\n",
    "                \"       n.description AS description, \"\n",
    "                \"       n.total_view_count AS total_view_count, \"\n",
    "                \"       n.followers AS followers\",\n",
    "                ids=list(node_ids)\n",
    "            )\n",
    "\n",
    "            # Build lookup dictionary\n",
    "            node_props = {}\n",
    "            for record in result:\n",
    "                node_props[record['id']] = {\n",
    "                    'id': record['id'],\n",
    "                    'description': record['description'],\n",
    "                    'total_view_count': record['total_view_count'],\n",
    "                    'followers': record['followers']\n",
    "                }\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    # Compute string similarity for each pair\n",
    "    string_sim_results = []\n",
    "    for _, row in high_similarity_df.iterrows():\n",
    "        node1_id = row['node1_id']\n",
    "        node2_id = row['node2_id']\n",
    "\n",
    "        if node1_id in node_props and node2_id in node_props:\n",
    "            props1 = node_props[node1_id]\n",
    "            props2 = node_props[node2_id]\n",
    "\n",
    "            # Compute string similarity for each property\n",
    "            prop_similarities = {}\n",
    "            for prop in STRING_PROPERTIES:\n",
    "                if prop in props1 and prop in props2:\n",
    "                    prop_similarities[f'{prop}_similarity'] = string_similarity(props1[prop], props2[prop])\n",
    "\n",
    "            string_sim_results.append({\n",
    "                'node1_id': node1_id,\n",
    "                'node2_id': node2_id,\n",
    "                'structural_similarity': row['similarity'],\n",
    "                'component_id': row['node1_component'],\n",
    "                **prop_similarities,\n",
    "                'node1_description': props1.get('description', '')[:50] if props1.get('description') else '',\n",
    "                'node2_description': props2.get('description', '')[:50] if props2.get('description') else ''\n",
    "            })\n",
    "\n",
    "    # Create DataFrame\n",
    "    string_sim_df = pd.DataFrame(string_sim_results)\n",
    "\n",
    "    print(f\"\\nâœ… Computed string similarity for {len(string_sim_df)} pairs\")\n",
    "    print(f\"\\nTop 20 pairs by combined similarity:\")\n",
    "\n",
    "    # Add average string similarity column\n",
    "    sim_cols = [col for col in string_sim_df.columns if col.endswith('_similarity') and col != 'structural_similarity']\n",
    "    if sim_cols:\n",
    "        string_sim_df['avg_string_similarity'] = string_sim_df[sim_cols].mean(axis=1)\n",
    "        display(string_sim_df.nlargest(20, 'avg_string_similarity'))\n",
    "    else:\n",
    "        display(string_sim_df.head(20))\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  No pairs found with similarity >= {SIMILARITY_THRESHOLD}\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Part 4: Node Embeddings\n",
    "\n",
    "**Algorithm**: FastRP (Fast Random Projection) creates vector representations of nodes, followed by KNN (K-Nearest Neighbors) to find similar nodes.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `GRAPH_NAME`: GDS graph projection name\n",
    "- `LABELS`: Node labels to include (e.g., ['Stream', 'Game'])\n",
    "- `REL_TYPES`: Relationship types to consider (e.g., ['MODERATOR', 'VIP', 'CHATTER'])\n",
    "- `embeddingDimension`: Size of embedding vectors (128)\n",
    "- `topK`: Number of nearest neighbors to find (10)\n",
    "\n",
    "**What it does:**\n",
    "- Creates a graph projection filtered by component_id_2\n",
    "- Generates 128-dimensional embeddings using FastRP\n",
    "- Finds top 10 most similar nodes for each node using KNN\n",
    "- Returns node pairs with similarity scores and component IDs"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Node Embeddings Configuration\n",
    "# EMBEDDING_GRAPH_NAME = \"stream-user-team-graph\"\n",
    "# EMBEDDING_LABELS = [\"Stream\", \"Steam\"]\n",
    "# EMBEDDING_DIMENSION = 128\n",
    "# TOP_K = 10\n",
    "\n",
    "# print(f\"Running Node Embeddings on graph: {EMBEDDING_GRAPH_NAME}\")\n",
    "# print(f\"Node labels: {EMBEDDING_LABELS}\")\n",
    "# print(f\"Embedding dimension: {EMBEDDING_DIMENSION}\")\n",
    "# print(f\"Top K neighbors: {TOP_K}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Step 1: Create Graph Projection for Embeddings (with component_id_2 filter)\n",
    "# driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "# with driver:\n",
    "#     with driver.session() as session:\n",
    "#         # Check if graph exists\n",
    "#         result = session.run(\n",
    "#             \"CALL gds.graph.exists($name) YIELD exists RETURN exists\",\n",
    "#             name=EMBEDDING_GRAPH_NAME\n",
    "#         )\n",
    "#         graph_exists = result.single()[\"exists\"]\n",
    "\n",
    "#         if graph_exists:\n",
    "#             print(f\"âœ… Graph '{EMBEDDING_GRAPH_NAME}' already exists - skipping creation\")\n",
    "#         else:\n",
    "#             print(f\"ðŸ“Š Creating graph projection '{EMBEDDING_GRAPH_NAME}' with component_id_2 filter...\")\n",
    "\n",
    "#             # Create Cypher projection filtering nodes with component_id_2\n",
    "#             session.run(\n",
    "#                 \"CALL gds.graph.project.cypher( \"\n",
    "#                 \"  $name, \"\n",
    "#                 \"  'MATCH (n) WHERE n.component_id_2 IS NOT NULL AND (n:Stream OR n:Game) RETURN id(n) AS id, labels(n) AS labels', \"\n",
    "#                 \"  'MATCH (n)-[r:MODERATOR|VIP|CHATTER]-(m) WHERE n.component_id_2 IS NOT NULL AND m.component_id_2 IS NOT NULL RETURN id(n) AS source, id(m) AS target, type(r) AS type' \"\n",
    "#                 \")\",\n",
    "#                 name=EMBEDDING_GRAPH_NAME\n",
    "#             )\n",
    "#             print(f\"âœ… Graph projection created\")\n",
    "\n",
    "#         # Show graph info\n",
    "#         info = session.run(\n",
    "#             \"CALL gds.graph.list($name) \"\n",
    "#             \"YIELD graphName, nodeCount, relationshipCount \"\n",
    "#             \"RETURN graphName, nodeCount, relationshipCount\",\n",
    "#             name=EMBEDDING_GRAPH_NAME\n",
    "#         ).single()\n",
    "\n",
    "#         if info:\n",
    "#             print(f\"\\nðŸ“Š Graph Statistics:\")\n",
    "#             print(f\"  Nodes: {info['nodeCount']:,}\")\n",
    "#             print(f\"  Relationships: {info['relationshipCount']:,}\")\n",
    "\n",
    "# driver.close()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Step 2: Run FastRP to Generate Embeddings\n",
    "# driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "# with driver:\n",
    "#     with driver.session() as session:\n",
    "#         print(f\"ðŸš€ Running FastRP algorithm...\")\n",
    "#         print(f\"  Embedding dimension: {EMBEDDING_DIMENSION}\")\n",
    "\n",
    "#         # Run FastRP and mutate the graph with embeddings\n",
    "#         result = session.run(\n",
    "#             \"CALL gds.fastRP.mutate($name, { \"\n",
    "#             \"  embeddingDimension: $dimension, \"\n",
    "#             \"  mutateProperty: 'embedding' \"\n",
    "#             \"}) \"\n",
    "#             \"YIELD nodePropertiesWritten\",\n",
    "#             name=EMBEDDING_GRAPH_NAME,\n",
    "#             dimension=EMBEDDING_DIMENSION\n",
    "#         )\n",
    "\n",
    "#         stats = result.single()\n",
    "#         print(f\"âœ… FastRP completed: {stats['nodePropertiesWritten']:,} embeddings created\")\n",
    "\n",
    "# driver.close()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 3: Run KNN on Embeddings\n",
    "# driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "# with driver:\n",
    "#     with driver.session() as session:\n",
    "#         print(f\"ðŸ” Running KNN algorithm (topK={TOP_K})...\")\n",
    "\n",
    "#         result = session.run(\n",
    "#             \"CALL gds.knn.stream($name, { \"\n",
    "#             \"  nodeProperties: ['embedding'], \"\n",
    "#             \"  topK: $topK \"\n",
    "#             \"}) \"\n",
    "#             \"YIELD node1, node2, similarity \"\n",
    "#             \"RETURN gds.util.asNode(node1).id AS node1_id, \"\n",
    "#             \"       gds.util.asNode(node1).component_id_2 AS component_id_node_1, \"\n",
    "#             \"       gds.util.asNode(node2).id AS node2_id, \"\n",
    "#             \"       gds.util.asNode(node2).component_id_2 AS component_id_node_2, \"\n",
    "#             \"       similarity \"\n",
    "#             \"ORDER BY similarity DESC \"\n",
    "#             \"LIMIT 1000\",\n",
    "#             name=EMBEDDING_GRAPH_NAME,\n",
    "#             topK=TOP_K,\n",
    "#         )\n",
    "\n",
    "#         # Collect results\n",
    "#         embedding_results = []\n",
    "#         for record in result:\n",
    "#             embedding_results.append({\n",
    "#                 'node1_id': record['node1_id'],\n",
    "#                 'component_id_1': record['component_id_node_1'],\n",
    "#                 'node2_id': record['node2_id'],\n",
    "#                 'component_id_2': record['component_id_node_2'],\n",
    "#                 'similarity': record['similarity']\n",
    "#             })\n",
    "\n",
    "# driver.close()\n",
    "\n",
    "# # Display results\n",
    "# embedding_df = pd.DataFrame(embedding_results)\n",
    "# print(f\"\\nâœ… Found {len(embedding_df)} embedding-based similar pairs (showing top 1000)\")\n",
    "# display(embedding_df.head(20))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.1 Visualize Embedding-Based Similarity"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize embedding similarity scores\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# # Histogram of embedding similarity scores\n",
    "# plt.hist(embedding_df['similarity'], bins=50, edgecolor='black', alpha=0.7, label='Embeddings')\n",
    "# if len(similarity_df) > 0:\n",
    "#     plt.hist(similarity_df['similarity'], bins=50, edgecolor='black', alpha=0.5, label='Node Similarity')\n",
    "# plt.xlabel('Similarity Score')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Comparison: Embedding vs Node Similarity')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"\\nEmbedding Similarity Statistics:\")\n",
    "# print(f\"  Mean similarity: {embedding_df['similarity'].mean():.4f}\")\n",
    "# print(f\"  Median similarity: {embedding_df['similarity'].median():.4f}\")\n",
    "# print(f\"  Max similarity: {embedding_df['similarity'].max():.4f}\")\n",
    "# print(f\"  Min similarity: {embedding_df['similarity'].min():.4f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "This notebook demonstrated a complete graph analysis pipeline:\n",
    "\n",
    "1. **EDA**: Explored the graph structure, node labels, and property distributions\n",
    "2. **WCC**: Identified connected components and assigned component_id_2 to nodes\n",
    "3. **Node Similarity**: Found similar nodes based on Jaccard similarity of shared neighbors\n",
    "4. **Node Embeddings**: Created vector representations and found similar nodes using KNN\n",
    "\n",
    "## Key Findings:\n",
    "- Component IDs help group related nodes together\n",
    "- Node similarity captures structural similarity based on shared connections\n",
    "- Embeddings capture deeper semantic relationships in the graph\n",
    "- Both approaches complement each other for comprehensive similarity analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
