{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY__H2bSfqkV"
      },
      "source": [
        "# Entity Resolution Data ‚Üí Neo4j KG\n",
        "\n",
        "This notebook follows the ERKG flow (datasets ‚Üí graph): load entity-resolution outputs, build nodes, and add relationships in Neo4j.\n",
        "\n",
        "Update the **Configuration** section with your paths and Neo4j credentials before running."
      ],
      "id": "dY__H2bSfqkV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup for Google Colab (skip if running locally)\n",
        "try:\n",
        "    import google.colab\n",
        "    import os\n",
        "    print(\"üì¶ Setting up Colab environment...\\n\")\n",
        "    GITHUB_URL = \"https://github.com/teutaD/EntityMatching-experimentation.git\"\n",
        "\n",
        "    # Clone repository\n",
        "    !git clone {GITHUB_URL}\n",
        "\n",
        "    # Check if clone was successful\n",
        "    if os.path.exists('EntityMatching-experimentation'):\n",
        "        os.chdir('EntityMatching-experimentation')\n",
        "        print(\"‚úÖ Repository cloned successfully\\n\")\n",
        "    else:\n",
        "        raise Exception(\"Failed to clone repository. Make sure the URL is correct and the repo is public.\")\n",
        "except ImportError:\n",
        "    print(\"‚úÖ Running locally - no setup needed\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during setup: {e}\")\n",
        "    print(\"\\nüí° Solutions:\")\n",
        "    print(\"   1. Make sure your GitHub repository is PUBLIC\")\n",
        "    print(\"   2. Or use a personal access token for private repos (see Option 2 above)\")\n",
        "    print(\"   3. Or manually upload the project files using Colab's file browser\")"
      ],
      "metadata": {
        "id": "RA_Z4Y-ZqNHK",
        "outputId": "272ac941-6dac-45dc-965c-76b7613c8551",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RA_Z4Y-ZqNHK",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Setting up Colab environment...\n",
            "\n",
            "Cloning into 'EntityMatching-experimentation'...\n",
            "remote: Enumerating objects: 123, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 123 (delta 36), reused 105 (delta 21), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (123/123), 900.16 KiB | 13.24 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n",
            "‚úÖ Repository cloned successfully\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "y3TdVf8grJXc",
        "outputId": "b79e4ce5-3206-4419-9c02-6fd51de8687d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "y3TdVf8grJXc",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "common\t\t\t\t  knn_results.txt\n",
            "data\t\t\t\t  node-similarity\n",
            "dim_reduction\t\t\t  node_similarity.txt\n",
            "eda\t\t\t\t  ontology_alignment_experiments.ipynb\n",
            "eda_results_20260209_124355.json  performance_report_20260209_124541.txt\n",
            "eda_results_20260209_124541.json  performance_report_20260210_140050.txt\n",
            "embeddings\t\t\t  retrieval_aligner.py\n",
            "fuzzy_matching.py\t\t  wcc\n",
            "gds_property_projection\t\t  WORKFLOW.md\n",
            "KGEmbeddings.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u4PheRBfqkV"
      },
      "source": [
        "## Configuration\n",
        "Set the file paths and Neo4j connection info. Defaults mirror the ERKG notebooks (local Neo4j Desktop)."
      ],
      "id": "0u4PheRBfqkV"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NW0MGTZVfqkV",
        "outputId": "cdf001aa-6fde-4600-dffd-0e46b1154437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/export.json'),\n",
              " PosixPath('data/raw'),\n",
              " 'neo4j+s://70772b8c.databases.neo4j.io')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "BASE_DIR = Path('.')\n",
        "EXPORT_JSON = Path(os.getenv('ER_EXPORT_JSON', 'data/export.json'))  # Senzing export\n",
        "RAW_DATA_DIR = Path(os.getenv('ER_RAW_DATA_DIR', 'data/raw'))        # optional raw inputs\n",
        "\n",
        "# Neo4j connection (match ERKG notebooks defaults)\n",
        "from google.colab import userdata\n",
        "\n",
        "NEO4J_URI = userdata.get('NEO4J_CONNECTION_URL',)\n",
        "NEO4J_USER = userdata.get('NEO4J_USER')\n",
        "NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')  # change me\n",
        "\n",
        "EXPORT_JSON, RAW_DATA_DIR, NEO4J_URI"
      ],
      "id": "NW0MGTZVfqkV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36UdnpoOfqkW"
      },
      "source": [
        "## Neo4j Helpers\n",
        "Create a small helper to run Cypher from Python (similar pattern to ERKG `graph.ipynb`)."
      ],
      "id": "36UdnpoOfqkW"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neo4j"
      ],
      "metadata": {
        "id": "kELtHSJYgHir",
        "outputId": "9b6a9b8e-30d1-45ed-a83f-de037a07156d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kELtHSJYgHir",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.12/dist-packages (6.1.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from neo4j) (2025.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kwGPgDu7fqkW",
        "outputId": "d80ea273-238f-460d-9a72-ef5daaf91b91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Record ok=1>]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "def run_cypher(query, params=None):\n",
        "    params = params or {}\n",
        "    with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD)) as driver:\n",
        "        with driver.session() as session:\n",
        "            return list(session.run(query, params))\n",
        "\n",
        "# Quick connectivity check\n",
        "run_cypher('RETURN 1 AS ok')"
      ],
      "id": "kwGPgDu7fqkW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFIUZR_efqkW"
      },
      "source": [
        "## Load Senzing Export\n",
        "The Senzing export is typically a JSON array or JSONL. This loader handles either. Adjust if your export format differs."
      ],
      "id": "iFIUZR_efqkW"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ckByilC2fqkW",
        "outputId": "2dd8e9c7-5152-44f9-c6d0-2d7d1af3c437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/export.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3638268597.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPORT_JSON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3638268597.py\u001b[0m in \u001b[0;36mload_export\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtext_stripped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtext_stripped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'['\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \"\"\"\n\u001b[1;32m   1026\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/export.json'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def load_export(path: Path):\n",
        "    text = path.read_text(encoding='utf-8')\n",
        "    text_stripped = text.lstrip()\n",
        "    if text_stripped.startswith('['):\n",
        "        return json.loads(text)\n",
        "    # assume JSON Lines\n",
        "    rows = []\n",
        "    for line in text.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "entities = load_export(EXPORT_JSON)\n",
        "len(entities)"
      ],
      "id": "ckByilC2fqkW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMX9Sv5KfqkW"
      },
      "source": [
        "## Normalize Records\n",
        "Convert Senzing entities into node/relationship payloads.\n",
        "This normalization is intentionally flexible: it looks for common Senzing fields like `ENTITY_ID`, `RECORDS`, `FEATURES`, etc.\n",
        "If your export schema differs, adjust the mapping here."
      ],
      "id": "MMX9Sv5KfqkW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xK3nO85fqkW"
      },
      "outputs": [],
      "source": [
        "def norm_entities(rows):\n",
        "    entity_rows = []\n",
        "    record_rows = []\n",
        "    name_rows = []\n",
        "    addr_rows = []\n",
        "    phone_rows = []\n",
        "\n",
        "    for e in rows:\n",
        "        entity_id = e.get('ENTITY_ID') or e.get('ENTITY_ID_STR') or e.get('entity_id')\n",
        "        if entity_id is None:\n",
        "            continue\n",
        "\n",
        "        entity_rows.append({\n",
        "            'entity_id': str(entity_id),\n",
        "            'resolved': True,\n",
        "            'score': e.get('MATCH_LEVEL') or e.get('match_level')\n",
        "        })\n",
        "\n",
        "        for r in e.get('RECORDS', []) or e.get('records', []):\n",
        "            record_id = r.get('RECORD_ID') or r.get('record_id') or r.get('RECORD_ID_STR')\n",
        "            data_source = r.get('DATA_SOURCE') or r.get('data_source')\n",
        "\n",
        "            record_rows.append({\n",
        "                'record_id': str(record_id),\n",
        "                'data_source': data_source,\n",
        "                'entity_id': str(entity_id),\n",
        "            })\n",
        "\n",
        "            # Common Senzing record features\n",
        "            for f in r.get('FEATURES', []) or r.get('features', []):\n",
        "                ftype = f.get('FEAT_TYPE') or f.get('feat_type')\n",
        "                fvals = f.get('FEAT_VALUES') or f.get('feat_values') or []\n",
        "                if ftype == 'NAME':\n",
        "                    for v in fvals:\n",
        "                        name_rows.append({\n",
        "                            'record_id': str(record_id),\n",
        "                            'name': v.get('NAME_FULL') or v.get('name_full') or v.get('name')\n",
        "                        })\n",
        "                elif ftype == 'ADDRESS':\n",
        "                    for v in fvals:\n",
        "                        addr_rows.append({\n",
        "                            'record_id': str(record_id),\n",
        "                            'address': v.get('ADDR_FULL') or v.get('addr_full') or v.get('address')\n",
        "                        })\n",
        "                elif ftype == 'PHONE':\n",
        "                    for v in fvals:\n",
        "                        phone_rows.append({\n",
        "                            'record_id': str(record_id),\n",
        "                            'phone': v.get('PHONE_NUMBER') or v.get('phone_number') or v.get('phone')\n",
        "                        })\n",
        "\n",
        "    return entity_rows, record_rows, name_rows, addr_rows, phone_rows\n",
        "\n",
        "entity_rows, record_rows, name_rows, addr_rows, phone_rows = norm_entities(entities)\n",
        "len(entity_rows), len(record_rows)"
      ],
      "id": "7xK3nO85fqkW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl6fq5NafqkW"
      },
      "source": [
        "## Neo4j Schema\n",
        "Create constraints and indexes for fast merge."
      ],
      "id": "dl6fq5NafqkW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeaqpnDxfqkW"
      },
      "outputs": [],
      "source": [
        "schema_cypher = [\n",
        "    'CREATE CONSTRAINT entity_id_unique IF NOT EXISTS FOR (e:Entity) REQUIRE e.entity_id IS UNIQUE',\n",
        "    'CREATE CONSTRAINT record_id_unique IF NOT EXISTS FOR (r:Record) REQUIRE r.record_id IS UNIQUE',\n",
        "    'CREATE INDEX name_value IF NOT EXISTS FOR (n:Name) ON (n.value)',\n",
        "    'CREATE INDEX address_value IF NOT EXISTS FOR (a:Address) ON (a.value)',\n",
        "    'CREATE INDEX phone_value IF NOT EXISTS FOR (p:Phone) ON (p.value)',\n",
        "]\n",
        "for stmt in schema_cypher:\n",
        "    run_cypher(stmt)\n",
        "\n",
        "'Schema ready'"
      ],
      "id": "jeaqpnDxfqkW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RRZxfKTfqkX"
      },
      "source": [
        "## Load Entities and Records\n",
        "Batch insert nodes and relationships. Adjust batch size for your environment."
      ],
      "id": "0RRZxfKTfqkX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nRQrpSyfqkX"
      },
      "outputs": [],
      "source": [
        "def chunks(rows, size=1000):\n",
        "    for i in range(0, len(rows), size):\n",
        "        yield rows[i:i+size]\n",
        "\n",
        "entity_cypher = '''\n",
        "UNWIND $rows AS row\n",
        "MERGE (e:Entity {entity_id: row.entity_id})\n",
        "SET e.resolved = row.resolved, e.score = row.score\n",
        "'''\n",
        "\n",
        "record_cypher = '''\n",
        "UNWIND $rows AS row\n",
        "MERGE (r:Record {record_id: row.record_id})\n",
        "SET r.data_source = row.data_source\n",
        "WITH r, row\n",
        "MATCH (e:Entity {entity_id: row.entity_id})\n",
        "MERGE (r)-[:RESOLVED_TO]->(e)\n",
        "'''\n",
        "\n",
        "for batch in chunks(entity_rows, 1000):\n",
        "    run_cypher(entity_cypher, {'rows': batch})\n",
        "\n",
        "for batch in chunks(record_rows, 1000):\n",
        "    run_cypher(record_cypher, {'rows': batch})\n",
        "\n",
        "'Entities and records loaded'"
      ],
      "id": "6nRQrpSyfqkX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUeWY00FfqkX"
      },
      "source": [
        "## Add Attribute Nodes and Relationships\n",
        "Create `Name`, `Address`, and `Phone` nodes, then connect them to `Record` nodes.\n",
        "If your ERKG process uses different feature types, extend this section."
      ],
      "id": "IUeWY00FfqkX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGFtapVRfqkX"
      },
      "outputs": [],
      "source": [
        "name_cypher = '''\n",
        "UNWIND $rows AS row\n",
        "WITH row WHERE row.name IS NOT NULL AND row.name <> ''\n",
        "MERGE (n:Name {value: row.name})\n",
        "WITH n, row\n",
        "MATCH (r:Record {record_id: row.record_id})\n",
        "MERGE (r)-[:HAS_NAME]->(n)\n",
        "'''\n",
        "\n",
        "addr_cypher = '''\n",
        "UNWIND $rows AS row\n",
        "WITH row WHERE row.address IS NOT NULL AND row.address <> ''\n",
        "MERGE (a:Address {value: row.address})\n",
        "WITH a, row\n",
        "MATCH (r:Record {record_id: row.record_id})\n",
        "MERGE (r)-[:HAS_ADDRESS]->(a)\n",
        "'''\n",
        "\n",
        "phone_cypher = '''\n",
        "UNWIND $rows AS row\n",
        "WITH row WHERE row.phone IS NOT NULL AND row.phone <> ''\n",
        "MERGE (p:Phone {value: row.phone})\n",
        "WITH p, row\n",
        "MATCH (r:Record {record_id: row.record_id})\n",
        "MERGE (r)-[:HAS_PHONE]->(p)\n",
        "'''\n",
        "\n",
        "for batch in chunks(name_rows, 2000):\n",
        "    run_cypher(name_cypher, {'rows': batch})\n",
        "\n",
        "for batch in chunks(addr_rows, 2000):\n",
        "    run_cypher(addr_cypher, {'rows': batch})\n",
        "\n",
        "for batch in chunks(phone_rows, 2000):\n",
        "    run_cypher(phone_cypher, {'rows': batch})\n",
        "\n",
        "'Attributes linked'"
      ],
      "id": "NGFtapVRfqkX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNoLvM2CfqkX"
      },
      "source": [
        "## Sanity Checks\n",
        "A couple of quick counts to verify the KG is populated."
      ],
      "id": "WNoLvM2CfqkX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpdMU4_efqkX"
      },
      "outputs": [],
      "source": [
        "run_cypher('MATCH (e:Entity) RETURN count(e) AS entities')\n",
        "run_cypher('MATCH (r:Record) RETURN count(r) AS records')\n",
        "run_cypher('MATCH (:Record)-[rel:RESOLVED_TO]->(:Entity) RETURN count(rel) AS resolved_edges')"
      ],
      "id": "MpdMU4_efqkX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiWCmNxFfqkX"
      },
      "source": [
        "## Notes\n",
        "- If your Senzing export uses different feature names, adjust `norm_entities()`.\n",
        "- If you want to reproduce the ERKG visualization, follow the analysis steps in `impact.ipynb`."
      ],
      "id": "TiWCmNxFfqkX"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}